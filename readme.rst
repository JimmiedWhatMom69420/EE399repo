.. role:: r(raw)
    :format: latex html
    
EE399 - Marcel Ramirez
=========

EE399 has a whole repository (in the near future) consisting of all projects pertaining this class. EE399 is a machine learning class, with a focus on linear regression models, gradience, and optimization problems.

.. contents:: Projects (table of contents):

Project 1 - Least Squares Error
---------------------
Below is the data set given. This is 31 sets of points for interpretation and modeling the data with least-squares error formula in 4 different perspectives:
X=np.arange(0,31)
Y=np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,
40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])
Fit the following model to the data with least-squares error 

.. code-block:: latex

$$E=\\sqrt{\\frac{1}{n} \\sum^n_{j=1} (f(x_j)-y_j)^2))$$

Introduction and Theory
^^^^^^^^^^^^
Before approaching the first question, the least squares error formula is a mathematical formula used to determine the best-fitting line or curve to a set of data points. The formula works by minimizing the sum of the squared differences between the actual data points and the predicted values generated by the mathematical function being fitted. This is done by adjusting the parameters of the function until the sum of the squared differences is as small as possible. The goal of minimizing the sum of squares is to find the values of the parameters that provide the best possible fit to the data, in the sense that the predicted values are as close as possible to the actual data points. 

The least squares error formula comes from polynomial regression, which is used for curve fitting a model in relationship between a dependent variable and one or more independent variables. The idea is for the set of points in the data for the polynomial function given, and to use this function for future applications. Utilizing a couple polynomial models, we compare how well they test for a sample dataset. There are three different algorithms that can be used to implement: a linear, quadratic, and a high-degree regression algorithm.

Part (I)
^^^^^^^^^^^^
First question prompts to write a code to find the minimum error and determine the parameters A, B, C, D. 

The code and graph produced a model to fit a given set of data using the least-squares error method. Specifically, the code fitted a model function f(x) = A cos(Bx) + Cx + D to a set of 31 data points (X, Y), where X ranges from 0 to 30 and Y represents the output variable we want to predict.

The graph shows the data points (blue dots) and the model function (orange line) plotted against the input variable X. The closer the line is to the data points, the better the model fits the data. In this case, we can see that the model fits the data reasonably well, as the line passes through or near most of the data points.

The parameters of the model (A, B, C, and D) were determined by minimizing the least-squares error, which is the sum of the squared differences between the predicted values of Y and the actual values of Y for each data point. The code used the Scipy library's curve_fit function to find the optimal values of the parameters that minimize the error.

Overall, the code and graph provide a visualization of the relationship between the input variable X and the output variable Y, and demonstrate how a model can be used to predict future values of Y for new values of X. The accuracy of the model can be evaluated by comparing its predictions to new data points not used in the model fitting process.

Part (II)
^^^^^^^^^^^^
Second question asks to use the results from the first question and fix two of the parameters and sweep through values of the other two parameters to generate a 2D loss (error) landscape. Next, I am to do all combinations of two fixed parameters and two swept parameters. I can also use something like pcolor to visualize the results in a grid. Finally, I need to determine how many minima can you find as you sweep through parameters according to the graphs?

In part (ii), the code generates a 2D loss landscape by sweeping through two of the parameters (A and B) while fixing the other two (C and D). The code also generates a second 2D loss landscape by sweeping through the other two parameters (C and D) while fixing the remaining two (A and B). In each case, the error is calculated for each combination of parameter values and stored in a 2D array. The results are then visualized using the matplotlib.pyplot.pcolor() function to create a heat map of the error values for each combination of parameter values. The code utilizes the function 'func()' function, to take in four parameters (A, B, C, D) and returns the value of the function for a given input 'x'. Data is defined as an array 'Y' of values for the function at different input values 'X'.

The first 2D loss landscape shows the error values as a function of the parameters A and B, while C and D are fixed. The second 2D loss landscape shows the error values as a function of the parameters C and D, while A and B are fixed. The heat maps show regions of low and high error, indicating the presence of minima and maxima in the loss landscape. The number of minima that can be found as parameters are swept through depends on the specific function being fit and the values chosen for the parameter ranges. The number of minima in these landscapes depends on the specific values chosen for the parameters. 

Since there are two fixed parameters and two that are being sweeped to generate a 2D loss (error) landscape for each combination of fixed parameters, there are 6 possible combinations of these from the four parameters A, B, C, D. I will generate 6 different 2D loss landscapes from these. The number of minima you can find as you sweep through parameters will depend on the specific parameter values you choose. It is possible to have multiple minima or a single minimum in each of the 2D landscapes. The code utilizes 'np.linspace()' to sweep through two parameters of choosing, and error values are calculated and stored in the 'error_vals' array using the same nested for loops and utilizing the least-squares error formula

Part (III & IIII)
^^^^^^^^^^^^
Question three wants to utilize the first 20 data points as training data, fit a line, parabola and 19th degree polynomial to the data. Compute the least-square error for each of these over the training points. Then compute the least square error of these models on the test data which are the remaining 10 data points.

The code first defines the data as two NumPy arrays, X and Y, which contain 31 data points. Then, it splits the data into training and test sets. The first 20 data points are used as the training set, and the remaining 10 data points are used as the test set.

Next, the code fits three different polynomial models to the training data using the 'np.polyfit' function from NumPy. Specifically, it fits a line (1st degree polynomial), a parabola (2nd degree polynomial), and a 19th-degree polynomial. For each model, the code computes the least-squares error on both the training and test sets using the np.sqrt and np.mean functions.

Finally, the code prints the least-squares errors for each model on the training and test sets and plots the training and test data along with the three fitted curves. The plot shows that the 19th-degree polynomial fits the training data extremely well, while the line and parabola fit less well. However, when we look at the test set errors, we see that the 19th-degree polynomial has the largest error, while the line has the smallest error. This suggests that the 19th-degree polynomial overfits the training data and does not generalize well to new data.

The math behind this code is the least-squares method, which is a mathematical technique for finding the best-fit curve or line for a given set of data points. The method involves minimizing the sum of the squares of the differences between the observed data and the predicted values of the curve. The np.polyfit function uses this method to find the coefficients of a polynomial that best fits the data points. The np.polyval function is then used to evaluate the polynomial at a given set of x-values.

Overall, the graph represents the training and test data points along with the fitted curves for a line, a parabola, and a 19th-degree polynomial. The least-squares errors for each model on the training and test sets are also displayed.

The final question follows the same steps as question three, but this time instead of using the first 20 data points as the training data, we use the first ten and the last ten data points as training data. Then, fitting the model to the test data, which would be the remaining 10 data points in the middle.

Conclusion of this assignment
^^^^^^^^^^^^
To sum up, the polynomial regression technique has the potential to effectively forecast data points, but selecting the appropriate degree of the polynomial relies on the characteristics of the dataset and its intended use. This particular implementation demonstrated that the linear and quadratic models were successful in predicting certain subsets of the data. However, the 19th degree polynomial model had a low training error but failed to perform well on the test data. Hence, additional exploration and experimentation are necessary to ascertain the most suitable degree of polynomial for a particular dataset.

Project 2 - Computing Correlation with Principal Components Analysis
---------------------

Overview
^^^^^^^^^^^^^^

This homework assignment focuses on exploring and analyzing a dataset of images using linear algebra techniques in Python. Specifically, the dataset provided is Yalefaces.mat, which consists of 39 faces with 65 lighting scenes each (a total of 2414 images) that have been downsampled to 32x32 grayscale images. The matrix X contains the image data and is of size 1024x2414.

The assignment consists of several parts. Part (a) requires computing a 100x100 correlation matrix by taking the dot product (correlation) between the first 100 images in X. Part (b) asks to identify the two most highly correlated and most uncorrelated images from the correlation matrix and plot them. Part (c) repeats part (a) but for a 10x10 correlation matrix and asks to plot the resulting matrix. Part (d) involves creating the matrix $$Y = XX^T$$ and finding the first six eigenvectors with the largest magnitude eigenvalue. Part (e) requires performing SVD on X and finding the first six principal component directions. Part (f) asks to compare the first eigenvector obtained in (d) with the first SVD mode obtained in (e) and computing the norm of the difference in their absolute values. Finally, part (g) requires computing the percentage of variance captured by each of the first six SVD modes and plotting these modes.

Overall, this assignment aims to provide an opportunity to practice using linear algebra techniques to analyze a dataset of images and gain insights into the relationships between different images in the dataset.

Theoretical Background and Implementation
^^^^^^^^^^^^

This assignment pertains to many linear algebra techniques utilized in image analysis and processing. Below are three key techniques:

#Correlation Matrix: A correlation matrix is a square matrix that contains the correlation coefficients between pairs of variables. In the context of images, the correlation matrix can be computed by taking the dot product (correlation) between pairs of image vectors. The resulting matrix provides a measure of the similarity or dissimilarity between pairs of images in the dataset.
.. code-block:: latex

$$c_{jk}=x^T_jX_k$$
Key Commands used to compute correlation matrix:
Part (A)
X_subset = X[:, :100] -> 100 x 100 matrix using the first 100 images 
C = np.dot(X_subset.T, X_subset) -> dot product

#Eigenvectors and Eigenvalues: Eigenvectors and eigenvalues are fundamental concepts in linear algebra that are commonly used in image processing and analysis. Eigenvectors are special vectors that are unchanged when a linear transformation is applied to them, except for scaling. Eigenvalues are scalars that represent the amount of scaling that occurs when a linear transformation is applied to an eigenvector. In image processing, eigenvectors and eigenvalues can be used to perform dimensionality reduction, image compression, and feature extraction.

.. code-block:: latex

$$Y = XX^T$$

#Singular Value Decomposition (SVD): SVD is a matrix decomposition technique that factorizes a matrix into three matrices: U, Σ, and V. U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of the original matrix. SVD is commonly used in image processing to perform dimensionality reduction, image compression, and feature extraction.

.. code-block:: latex

$$X = UΣV^T$$

.. code-block:: latex

$$XV = UΣ$$

* U and V are orthogonal matrices
* Σ is a diagonal matrix containing the singular values of the original matrix

Key commands:

* eigenvalues, eigenvectors = np.linalg.eig(Y) -> utilized to compute eigenvectors and eigenvalues of Y
* U, s, Vt = np.linalg.svd(X) -> compute the SVD of X

Computational Output
^^^^^^^^^^^^^^^^^^

Image below is a correlation matrix 100 x 100 using the first 100 images via dot product. It finds the difference of pictures taken of the same person but in several lighting conditions ranging light to dark.

.. figure:: /hw2partA.png

|

From the Correlation matrix calculated previous, the two highest correlated image pair are shown below:

.. figure:: /hw2partb1.png

From the Correlation matrix calculated previous, the two lowest correlated image pair are shown below:

.. figure:: /hw2partb2.png

For a very specific set of images given by the specification, (1, 313, 512, 5, 2400, 113, 1024, 87, 314, and 2005), images 87 and 314 are shown to be the most correlated to each other. The figure is shown below:

.. figure:: /hw2partc.png

The figure below displays six different pictures displaying the first six principal component directions computed by using SVD:

.. figure:: /hw2parte.png

There are a lot of variance displayed in the first six SVD modes shown. SVD appears to do a great job capturing commonly reoccurring facial features such as the mouth or the eyes. The variance of the first 6 SVD modes vary about 30%. Images of the first 6 SVD modes are shown below:

.. figure:: /hw2partg.png

Conclusion and Summary
^^^^^^^^^^^^^^^^^^

Correlation Matrices are very useful when finding the relationship between the variables and how close they are based on the different pixels that are in this image classificaiton. The results of the linear regression and polynomial regression show that the complexity of the model affects the performance on the test data, and overfitting can occur if the model is too complex. Singular Value Decomposition is also very useful when classifying images to reduce the dimensionality and extract certain features from the original data. Overall, this study provides valuable insights into the use of non-linear regression and different model complexities for fitting models to data.
