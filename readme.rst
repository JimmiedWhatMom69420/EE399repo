EE399 - Marcel Ramirez
=========

EE399 has a whole repository (in the near future) consisting of all projects pertaining this class. EE399 is a machine learning class, with a focus on linear regression models, gradience, and optimization problems.

.. contents:: Projects (table of contents):
Project 1 - Least Squares Error

Project 1 - Least Squares Error
---------------------
Below is the data set given. This is 31 sets of points for interpretation and modeling the data with least-squares error formula in 4 different perspectives:
X=np.arange(0,31)
Y=np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,
40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])
Fit the following model to the data with least-squares error 

.. code-block:: latex

E=\sqrt(1/n)\sum^n_(j=1) (f(x_j)-y_j)^2))

Introduction and Theory
^^^^^^^^^^^^
Before approaching the first question, the least squares error formula is a mathematical formula used to determine the best-fitting line or curve to a set of data points. The formula works by minimizing the sum of the squared differences between the actual data points and the predicted values generated by the mathematical function being fitted. This is done by adjusting the parameters of the function until the sum of the squared differences is as small as possible. The goal of minimizing the sum of squares is to find the values of the parameters that provide the best possible fit to the data, in the sense that the predicted values are as close as possible to the actual data points. 

The least squares error formula comes from polynomial regression, which is used for curve fitting a model in relationship between a dependent variable and one or more independent variables. The idea is for the set of points in the data for the polynomial function given, and to use this function for future applications. Utilizing a couple polynomial models, we compare how well they test for a sample dataset. There are three different algorithms that can be used to implement: a linear, quadratic, and a high-degree regression algorithm.

Part (I)
^^^^^^^^^^^^
First question prompts to write a code to find the minimum error and determine the parameters A, B, C, D. 

The code and graph produced a model to fit a given set of data using the least-squares error method. Specifically, the code fitted a model function f(x) = A cos(Bx) + Cx + D to a set of 31 data points (X, Y), where X ranges from 0 to 30 and Y represents the output variable we want to predict.

The graph shows the data points (blue dots) and the model function (orange line) plotted against the input variable X. The closer the line is to the data points, the better the model fits the data. In this case, we can see that the model fits the data reasonably well, as the line passes through or near most of the data points.

The parameters of the model (A, B, C, and D) were determined by minimizing the least-squares error, which is the sum of the squared differences between the predicted values of Y and the actual values of Y for each data point. The code used the Scipy library's curve_fit function to find the optimal values of the parameters that minimize the error.

Overall, the code and graph provide a visualization of the relationship between the input variable X and the output variable Y, and demonstrate how a model can be used to predict future values of Y for new values of X. The accuracy of the model can be evaluated by comparing its predictions to new data points not used in the model fitting process.

Part (II)
^^^^^^^^^^^^
Second question asks to use the results from the first question and fix two of the parameters and sweep through values of the other two parameters to generate a 2D loss (error) landscape. Next, I am to do all combinations of two fixed parameters and two swept parameters. I can also use something like pcolor to visualize the results in a grid. Finally, I need to determine how many minima can you find as you sweep through parameters according to the graphs?

In part (ii), the code generates a 2D loss landscape by sweeping through two of the parameters (A and B) while fixing the other two (C and D). The code also generates a second 2D loss landscape by sweeping through the other two parameters (C and D) while fixing the remaining two (A and B). In each case, the error is calculated for each combination of parameter values and stored in a 2D array. The results are then visualized using the matplotlib.pyplot.pcolor() function to create a heat map of the error values for each combination of parameter values. The code utilizes the function 'func()' function, to take in four parameters (A, B, C, D) and returns the value of the function for a given input 'x'. Data is defined as an array 'Y' of values for the function at different input values 'X'.

The first 2D loss landscape shows the error values as a function of the parameters A and B, while C and D are fixed. The second 2D loss landscape shows the error values as a function of the parameters C and D, while A and B are fixed. The heat maps show regions of low and high error, indicating the presence of minima and maxima in the loss landscape. The number of minima that can be found as parameters are swept through depends on the specific function being fit and the values chosen for the parameter ranges. The number of minima in these landscapes depends on the specific values chosen for the parameters. 

Since there are two fixed parameters and two that are being sweeped to generate a 2D loss (error) landscape for each combination of fixed parameters, there are 6 possible combinations of these from the four parameters A, B, C, D. I will generate 6 different 2D loss landscapes from these. The number of minima you can find as you sweep through parameters will depend on the specific parameter values you choose. It is possible to have multiple minima or a single minimum in each of the 2D landscapes. The code utilizes 'np.linspace()' to sweep through two parameters of choosing, and error values are calculated and stored in the 'error_vals' array using the same nested for loops and utilizing the least-squares error formula

Part (III & IIII)
^^^^^^^^^^^^
Question three wants to utilize the first 20 data points as training data, fit a line, parabola and 19th degree polynomial to the data. Compute the least-square error for each of these over the training points. Then compute the least square error of these models on the test data which are the remaining 10 data points.

The code first defines the data as two NumPy arrays, X and Y, which contain 31 data points. Then, it splits the data into training and test sets. The first 20 data points are used as the training set, and the remaining 10 data points are used as the test set.

Next, the code fits three different polynomial models to the training data using the 'np.polyfit' function from NumPy. Specifically, it fits a line (1st degree polynomial), a parabola (2nd degree polynomial), and a 19th-degree polynomial. For each model, the code computes the least-squares error on both the training and test sets using the np.sqrt and np.mean functions.

Finally, the code prints the least-squares errors for each model on the training and test sets and plots the training and test data along with the three fitted curves. The plot shows that the 19th-degree polynomial fits the training data extremely well, while the line and parabola fit less well. However, when we look at the test set errors, we see that the 19th-degree polynomial has the largest error, while the line has the smallest error. This suggests that the 19th-degree polynomial overfits the training data and does not generalize well to new data.

The math behind this code is the least-squares method, which is a mathematical technique for finding the best-fit curve or line for a given set of data points. The method involves minimizing the sum of the squares of the differences between the observed data and the predicted values of the curve. The np.polyfit function uses this method to find the coefficients of a polynomial that best fits the data points. The np.polyval function is then used to evaluate the polynomial at a given set of x-values.

Overall, the graph represents the training and test data points along with the fitted curves for a line, a parabola, and a 19th-degree polynomial. The least-squares errors for each model on the training and test sets are also displayed.

The final question follows the same steps as question three, but this time instead of using the first 20 data points as the training data, we use the first ten and the last ten data points as training data. Then, fitting the model to the test data, which would be the remaining 10 data points in the middle.

Conclusion of this assignment
^^^^^^^^^^^^
To sum up, the polynomial regression technique has the potential to effectively forecast data points, but selecting the appropriate degree of the polynomial relies on the characteristics of the dataset and its intended use. This particular implementation demonstrated that the linear and quadratic models were successful in predicting certain subsets of the data. However, the 19th degree polynomial model had a low training error but failed to perform well on the test data. Hence, additional exploration and experimentation are necessary to ascertain the most suitable degree of polynomial for a particular dataset.
