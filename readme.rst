EE399 - Marcel Ramirez
=========

EE399 has a whole repository (in the near future) consisting of all projects pertaining this class. EE399 is a machine learning class, with a focus on linear regression models, gradience, and optimization problems.

.. contents:: Projects (table of contents):

Project 1 - Least Squares Error
---------------------
Below is the data set given. This is 31 sets of points for interpretation and modeling the data with least-squares error formula in 4 different perspectives:
X=np.arange(0,31)
Y=np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,
40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])
Fit the following model to the data with least-squares error 

.. code-block:: latex

$$E=\\sqrt{\\frac{1}{n} \\sum^n_{j=1} (f(x_j)-y_j)^2))$$

Introduction and Theory
^^^^^^^^^^^^
Before approaching the first question, the least squares error formula is a mathematical formula used to determine the best-fitting line or curve to a set of data points. The formula works by minimizing the sum of the squared differences between the actual data points and the predicted values generated by the mathematical function being fitted. This is done by adjusting the parameters of the function until the sum of the squared differences is as small as possible. The goal of minimizing the sum of squares is to find the values of the parameters that provide the best possible fit to the data, in the sense that the predicted values are as close as possible to the actual data points. 

The least squares error formula comes from polynomial regression, which is used for curve fitting a model in relationship between a dependent variable and one or more independent variables. The idea is for the set of points in the data for the polynomial function given, and to use this function for future applications. Utilizing a couple polynomial models, we compare how well they test for a sample dataset. There are three different algorithms that can be used to implement: a linear, quadratic, and a high-degree regression algorithm.

Part (I)
^^^^^^^^^^^^
First question prompts to write a code to find the minimum error and determine the parameters A, B, C, D. 

The code and graph produced a model to fit a given set of data using the least-squares error method. Specifically, the code fitted a model function f(x) = A cos(Bx) + Cx + D to a set of 31 data points (X, Y), where X ranges from 0 to 30 and Y represents the output variable we want to predict.

The graph shows the data points (blue dots) and the model function (orange line) plotted against the input variable X. The closer the line is to the data points, the better the model fits the data. In this case, we can see that the model fits the data reasonably well, as the line passes through or near most of the data points.

The parameters of the model (A, B, C, and D) were determined by minimizing the least-squares error, which is the sum of the squared differences between the predicted values of Y and the actual values of Y for each data point. The code used the Scipy library's curve_fit function to find the optimal values of the parameters that minimize the error.

Overall, the code and graph provide a visualization of the relationship between the input variable X and the output variable Y, and demonstrate how a model can be used to predict future values of Y for new values of X. The accuracy of the model can be evaluated by comparing its predictions to new data points not used in the model fitting process.

Part (II)
^^^^^^^^^^^^
Second question asks to use the results from the first question and fix two of the parameters and sweep through values of the other two parameters to generate a 2D loss (error) landscape. Next, I am to do all combinations of two fixed parameters and two swept parameters. I can also use something like pcolor to visualize the results in a grid. Finally, I need to determine how many minima can you find as you sweep through parameters according to the graphs?

In part (ii), the code generates a 2D loss landscape by sweeping through two of the parameters (A and B) while fixing the other two (C and D). The code also generates a second 2D loss landscape by sweeping through the other two parameters (C and D) while fixing the remaining two (A and B). In each case, the error is calculated for each combination of parameter values and stored in a 2D array. The results are then visualized using the matplotlib.pyplot.pcolor() function to create a heat map of the error values for each combination of parameter values. The code utilizes the function 'func()' function, to take in four parameters (A, B, C, D) and returns the value of the function for a given input 'x'. Data is defined as an array 'Y' of values for the function at different input values 'X'.

The first 2D loss landscape shows the error values as a function of the parameters A and B, while C and D are fixed. The second 2D loss landscape shows the error values as a function of the parameters C and D, while A and B are fixed. The heat maps show regions of low and high error, indicating the presence of minima and maxima in the loss landscape. The number of minima that can be found as parameters are swept through depends on the specific function being fit and the values chosen for the parameter ranges. The number of minima in these landscapes depends on the specific values chosen for the parameters. 

Since there are two fixed parameters and two that are being sweeped to generate a 2D loss (error) landscape for each combination of fixed parameters, there are 6 possible combinations of these from the four parameters A, B, C, D. I will generate 6 different 2D loss landscapes from these. The number of minima you can find as you sweep through parameters will depend on the specific parameter values you choose. It is possible to have multiple minima or a single minimum in each of the 2D landscapes. The code utilizes 'np.linspace()' to sweep through two parameters of choosing, and error values are calculated and stored in the 'error_vals' array using the same nested for loops and utilizing the least-squares error formula

Part (III & IIII)
^^^^^^^^^^^^
Question three wants to utilize the first 20 data points as training data, fit a line, parabola and 19th degree polynomial to the data. Compute the least-square error for each of these over the training points. Then compute the least square error of these models on the test data which are the remaining 10 data points.

The code first defines the data as two NumPy arrays, X and Y, which contain 31 data points. Then, it splits the data into training and test sets. The first 20 data points are used as the training set, and the remaining 10 data points are used as the test set.

Next, the code fits three different polynomial models to the training data using the 'np.polyfit' function from NumPy. Specifically, it fits a line (1st degree polynomial), a parabola (2nd degree polynomial), and a 19th-degree polynomial. For each model, the code computes the least-squares error on both the training and test sets using the np.sqrt and np.mean functions.

Finally, the code prints the least-squares errors for each model on the training and test sets and plots the training and test data along with the three fitted curves. The plot shows that the 19th-degree polynomial fits the training data extremely well, while the line and parabola fit less well. However, when we look at the test set errors, we see that the 19th-degree polynomial has the largest error, while the line has the smallest error. This suggests that the 19th-degree polynomial overfits the training data and does not generalize well to new data.

The math behind this code is the least-squares method, which is a mathematical technique for finding the best-fit curve or line for a given set of data points. The method involves minimizing the sum of the squares of the differences between the observed data and the predicted values of the curve. The np.polyfit function uses this method to find the coefficients of a polynomial that best fits the data points. The np.polyval function is then used to evaluate the polynomial at a given set of x-values.

Overall, the graph represents the training and test data points along with the fitted curves for a line, a parabola, and a 19th-degree polynomial. The least-squares errors for each model on the training and test sets are also displayed.

The final question follows the same steps as question three, but this time instead of using the first 20 data points as the training data, we use the first ten and the last ten data points as training data. Then, fitting the model to the test data, which would be the remaining 10 data points in the middle.

Conclusion of this assignment
^^^^^^^^^^^^
To sum up, the polynomial regression technique has the potential to effectively forecast data points, but selecting the appropriate degree of the polynomial relies on the characteristics of the dataset and its intended use. This particular implementation demonstrated that the linear and quadratic models were successful in predicting certain subsets of the data. However, the 19th degree polynomial model had a low training error but failed to perform well on the test data. Hence, additional exploration and experimentation are necessary to ascertain the most suitable degree of polynomial for a particular dataset.

Project 2 - Computing Correlation with Principal Components Analysis
---------------------

Overview
^^^^^^^^^^^^^^

This homework assignment focuses on exploring and analyzing a dataset of images using linear algebra techniques in Python. Specifically, the dataset provided is Yalefaces.mat, which consists of 39 faces with 65 lighting scenes each (a total of 2414 images) that have been downsampled to 32x32 grayscale images. The matrix X contains the image data and is of size 1024x2414.

The assignment consists of several parts. Part (a) requires computing a 100x100 correlation matrix by taking the dot product (correlation) between the first 100 images in X. Part (b) asks to identify the two most highly correlated and most uncorrelated images from the correlation matrix and plot them. Part (c) repeats part (a) but for a 10x10 correlation matrix and asks to plot the resulting matrix. Part (d) involves creating the matrix $$Y = XX^T$$ and finding the first six eigenvectors with the largest magnitude eigenvalue. Part (e) requires performing SVD on X and finding the first six principal component directions. Part (f) asks to compare the first eigenvector obtained in (d) with the first SVD mode obtained in (e) and computing the norm of the difference in their absolute values. Finally, part (g) requires computing the percentage of variance captured by each of the first six SVD modes and plotting these modes.

Overall, this assignment aims to provide an opportunity to practice using linear algebra techniques to analyze a dataset of images and gain insights into the relationships between different images in the dataset.

Theoretical Background and Implementation
^^^^^^^^^^^^

This assignment pertains to many linear algebra techniques utilized in image analysis and processing. Below are three key techniques:

#Correlation Matrix: A correlation matrix is a square matrix that contains the correlation coefficients between pairs of variables. In the context of images, the correlation matrix can be computed by taking the dot product (correlation) between pairs of image vectors. The resulting matrix provides a measure of the similarity or dissimilarity between pairs of images in the dataset.

.. code-block:: latex

$$c_{jk}=x^T_jX_k$$

.. code-block:: latex

Key Commands used to compute correlation matrix:

.. code-block:: latex

Part (A)

.. code-block:: latex

* X_subset = X[:, :100] -> 100 x 100 matrix using the first 100 images 

.. code-block:: latex

* C = np.dot(X_subset.T, X_subset) -> dot product

#Eigenvectors and Eigenvalues: Eigenvectors and eigenvalues are fundamental concepts in linear algebra that are commonly used in image processing and analysis. Eigenvectors are special vectors that are unchanged when a linear transformation is applied to them, except for scaling. Eigenvalues are scalars that represent the amount of scaling that occurs when a linear transformation is applied to an eigenvector. In image processing, eigenvectors and eigenvalues can be used to perform dimensionality reduction, image compression, and feature extraction.

.. code-block:: latex

$$Y = XX^T$$

#Singular Value Decomposition (SVD): SVD is a matrix decomposition technique that factorizes a matrix into three matrices: U, Σ, and V. U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of the original matrix. SVD is commonly used in image processing to perform dimensionality reduction, image compression, and feature extraction.

.. code-block:: latex

$$X = UΣV^T$$

.. code-block:: latex

$$XV = UΣ$$

* U and V are orthogonal matrices
* Σ is a diagonal matrix containing the singular values of the original matrix

Key commands:

* eigenvalues, eigenvectors = np.linalg.eig(Y) -> utilized to compute eigenvectors and eigenvalues of Y
* U, s, Vt = np.linalg.svd(X) -> compute the SVD of X

Computational Output
^^^^^^^^^^^^^^^^^^

Image below is a correlation matrix 100 x 100 using the first 100 images via dot product. It finds the difference of pictures taken of the same person but in several lighting conditions ranging light to dark.

.. figure:: /hw2partA.png

|

From the Correlation matrix calculated previous, the two highest correlated image pair are shown below:

.. figure:: /hw2partb1.png

From the Correlation matrix calculated previous, the two lowest correlated image pair are shown below:

.. figure:: /hw2partb2.png

For a very specific set of images given by the specification, (1, 313, 512, 5, 2400, 113, 1024, 87, 314, and 2005), images 87 and 314 are shown to be the most correlated to each other. The figure is shown below:

.. figure:: /hw2partc.png

The figure below displays six different pictures displaying the first six principal component directions computed by using SVD:

.. figure:: /hw2parte.png

There are a lot of variance displayed in the first six SVD modes shown. SVD appears to do a great job capturing commonly reoccurring facial features such as the mouth or the eyes. The variance of the first 6 SVD modes vary about 30%. Images of the first 6 SVD modes are shown below:

.. figure:: /hw2partg.png

Conclusion and Summary
^^^^^^^^^^^^^^^^^^

Correlation Matrices are very useful when finding the relationship between the variables and how close they are based on the different pixels that are in this image classificaiton. The results of the linear regression and polynomial regression show that the complexity of the model affects the performance on the test data, and overfitting can occur if the model is too complex. Singular Value Decomposition is also very useful when classifying images to reduce the dimensionality and extract certain features from the original data. Overall, this study provides valuable insights into the use of non-linear regression and different model complexities for fitting models to data.

Project 3 - MNIST Data Set Analysis
---------------------

.. code-block:: latex

$$E=\\sqrt{\\frac{1}{n} \\sum^n_{j=1} (f(x_j)-y_j)^2))$$

Introduction and Theory or Overview
^^^^^^^^^^^^

This assignment is for the course EE 399 in Spring Quarter 2023 with instructor J. Nathan Kutz. The task is to analyze the MNIST dataset, which is a dataset of handwritten digits. The analysis involves performing an SVD analysis of the digit images, interpreting the singular value spectrum, projecting the data into PCA space and building a classifier to identify individual digits in the training set using LDA, SVM and decision tree classifiers.

The SVD analysis of the digit images requires the reshaping of each image into a column vector, and each column of the data matrix represents a different image. The analysis should determine the number of modes necessary for good image reconstruction and the interpretation of the U, Σ, and V matrices. On a 3D plot, the data should be projected onto three selected V-modes colored by their digit label.

The next step is to build a classifier to identify individual digits in the training set. The first task is to pick two digits and build a linear classifier (LDA) that can reasonably identify/classify them. The next step is to pick three digits and build a linear classifier to identify these three. The most difficult and most straightforward pairs of digits to separate should be identified, and the accuracy of separation with LDA on the test data should be quantified. The state-of-the-art classifiers SVM and decision tree should also be tested to see how well they separate all ten digits.

The performance of the classifiers on both the training and test sets should be discussed, and many pictures should be included.


Theoretical Background and Implementation
^^^^^^^^^^^^

Theoretical Background:
The MNIST dataset is a collection of 60,000 training and 10,000 testing images of handwritten digits. Each image is a 28x28 grayscale image, representing a digit from 0 to 9. The task is to build classifiers that can identify the digits in the images accurately.

Singular value decomposition (SVD) is a matrix factorization technique that is commonly used in data analysis. For a matrix A, SVD factorizes A into three matrices: U, Σ, and V such that A=UΣVᵀ. The matrix U contains the left singular vectors, the matrix V contains the right singular vectors, and the diagonal matrix Σ contains the singular values. The singular values represent the magnitude of the importance of each singular vector in the decomposition.

Principal component analysis (PCA) is a dimensionality reduction technique that uses SVD to project high-dimensional data onto a lower-dimensional space. PCA identifies the directions of maximum variance in the data and projects the data onto a smaller subspace spanned by these directions. In the case of image analysis, PCA can be used to identify the most important features of the images and project the images onto a lower-dimensional space, where they can be more easily classified.

Linear discriminant analysis (LDA) is a classification technique that finds the linear combinations of features that best separate the classes. The goal is to maximize the ratio of the between-class variance to the within-class variance. The resulting linear combinations can be used to classify new data points based on their features.

Support vector machines (SVMs) are a type of classifier that constructs a hyperplane or set of hyperplanes in a high-dimensional space that can be used for classification. The goal is to find the hyperplane that maximizes the margin between the two classes of data points. SVMs can handle both linearly separable and non-linearly separable data.

Decision trees are a classification technique that builds a tree-like model of decisions and their possible consequences. The tree is constructed by recursively splitting the data based on the values of the features until the leaves of the tree represent the classes of the data.

Image below is a representation of how data is split in a decision tree in a Matlab example: 

.. figure:: /Split.png

Implementation:
To implement the analysis of the MNIST dataset, the following steps should be taken:

Load the dataset and reshape each image into a column vector.
Perform an SVD analysis of the data matrix to identify the number of modes necessary for good image reconstruction and interpret the U, Σ, and V matrices.
Project the data onto PCA space using the selected V-modes and plot the results.
Build a linear classifier (LDA) to identify individual digits in the training set.
Test the performance of the LDA classifier on both the training and test sets.
Select two digits and build an LDA classifier to identify them.
Select three digits and build an LDA classifier to identify them.
Identify the most difficult and most straightforward pairs of digits to separate using LDA and quantify the accuracy of separation on the test data.
Implement SVM and decision tree classifiers and compare their performance with LDA on the most difficult and most straightforward pairs of digits to separate.
Discuss the performance of the classifiers on both the training and test sets and include many pictures to illustrate the results.
The implementation can be done using Python and libraries such as NumPy, Matplotlib, and Scikit-learn.


Part (I)
^^^^^^^^^^^^

SVD analysis of the MNIST data set:

$$import numpy as np$$
$$from sklearn.datasets import fetch_openml$$
$$from sklearn.decomposition import TruncatedSVD$$

# Load MNIST dataset
$$mnist = fetch_openml('mnist_784')$$

# Extract image and label data
$$X = mnist.data$$
$$y = mnist.target$$

# Reshape image data into column vectors
$$X = X.T$$

# Perform SVD analysis
$$svd = TruncatedSVD(n_components=10, random_state=42)$$
$$X_svd = svd.fit_transform(X)$$

# Print the explained variance ratios
print("Explained variance ratios:", svd.explained_variance_ratio_)
In this code, we first load the MNIST dataset using the fetch_openml function from the sklearn.datasets module. We then extract the image data X and label data y.

Next, we reshape the image data X into column vectors using the .T method, which transposes the matrix.

Then, we perform an SVD analysis using the TruncatedSVD class from the sklearn.decomposition module. We set the number of components to 10 and the random state to 42 for reproducibility.

Finally, we print the explained variance ratios using the explained_variance_ratio_ attribute of the TruncatedSVD object.

Important Concept - SVD Analysis:

Singular Value Decomposition (SVD) is a matrix factorization method that can be used to analyze and compress data. It decomposes a matrix into three matrices: U, Σ, and V, such that X = UΣVᵀ. U and V are orthogonal matrices, and Σ is a diagonal matrix containing the singular values of X.

The singular values represent the amount of variance in the data captured by each principal component. We can use the SVD to perform dimensionality reduction by selecting the top k singular values and their corresponding columns in U and V.

The image below shows an example of an SVM utilizing Covers Theorem:

.. figure:: /SVMEx.png


Important Commands:

fetch_openml('mnist_784'): This command loads the MNIST dataset from the OpenML database.
TruncatedSVD(n_components=k, random_state=seed): This command creates a TruncatedSVD object with k components and a random seed for reproducibility.
fit_transform(X): This command fits the TruncatedSVD object to the data X and returns the transformed data X_svd.
explained_variance_ratio_: This command returns the explained variance ratios of each principal component.


Performing an SVD analysis on the MNIST dataset allows us to identify the most important features and structure of the data. By selecting the top k principal components, we can reduce the dimensionality of the data and potentially improve the performance of machine learning models. The code above demonstrates how to perform an SVD analysis using Python and the scikit-learn library.

Important Commands:

fetch_openml('mnist_784'): This command loads the MNIST dataset from the OpenML database.
TruncatedSVD(n_components=k, random_state=seed): This command creates a TruncatedSVD object with k components and a random seed for reproducibility.
fit_transform(X): This command fits the TruncatedSVD object to the data X and returns the transformed data X_svd.
explained_variance_ratio_: This command returns the explained variance ratios of each principal component.

Conclusion:
Performing an SVD analysis on the MNIST dataset allows us to identify the most important features and structure of the data. By selecting the top k principal components, we can reduce the dimensionality of the data and potentially improve the performance of machine learning models. The code above demonstrates how to perform an SVD analysis using Python and the scikit-learn library.

Part (II)
^^^^^^^^^^^^

First, let's take a look at the singular value spectrum of the MNIST dataset. The singular value spectrum is a plot of the singular values of the data matrix in decreasing order. We can use it to determine the rank r of the digit space and how many modes are necessary for good image reconstruction.

Important Formulas and concepts utilized:

Singular Value Decomposition (SVD):

Singular value decomposition (SVD) is a technique used to decompose a matrix into three separate matrices. In this analysis, we used SVD to determine the rank of the digit space and how many modes are necessary for good image reconstruction.

The SVD of a matrix X is given by X = UΣV^T, where U and V are orthogonal matrices and Σ is a diagonal matrix of singular values. The singular values in Σ are sorted in decreasing order, and the rank of X is equal to the number of non-zero singular values.

The image below shows an example of an SVM utilizing Covers Theorem:

.. figure:: /SVMEx.png

Logistic Regression:

Logistic regression is a statistical method used to analyze a dataset in which there are one or more independent variables that determine an outcome. In this analysis, we used logistic regression to classify the images of handwritten digits into their corresponding labels.

Logistic regression works by modeling the probability of the binary outcome (0 or 1) using a logistic function. The logistic function is given by:

σ(z) = 1 / (1 + e^-z)

where z is a linear combination of the input features and their associated weights. The weights are learned from the training data using maximum likelihood estimation.


Important Commands:

fetch_openml: a function from scikit-learn that is used to load the MNIST dataset.
np.linalg.svd: a function from NumPy that performs the SVD of a matrix.
train_test_split: a function from scikit-learn that is used to split the dataset into a training set and a test set.
LogisticRegression: a class from scikit-learn that is used to train a logistic regression model.
accuracy_score, precision_score, recall_score, f1_score: functions from scikit-learn that are used to evaluate the performance of the model.


Here is a plot of the singular value spectrum of the MNIST dataset:

From the plot, we can see that the singular values decrease rapidly at first, then level off. This suggests that the rank of the digit space is relatively low, and that many of the singular values can be truncated without significant loss of information.

To determine the number of modes necessary for good image reconstruction, we can look at the rate at which the singular values decay. We can see that the first few singular values have a much larger magnitude than the rest, which indicates that the first few modes contain most of the important information.

Next, we can use a machine learning model to classify the images into their corresponding labels. For this analysis, we will use a simple logistic regression model. We will split the dataset into a training set and a test set, and evaluate the performance of the model on both sets.

Here are some performance metrics for the logistic regression model:

---------------------------------
Metric	Training Set	Test Set
Accuracy	0.9266	0.9219
Precision(weighted)	0.9266	0.9223
Recall (weighted)	0.9266	0.9219
F1 Score (weighted)	0.9266	0.9221

From these metrics, we can see that the performance of the model on the test set is very similar to its performance on the training set. This suggests that the model is not overfitting the training data.

In this analysis, we used singular value decomposition to determine the rank of the digit space and how many modes are necessary for good image reconstruction. We also used logistic regression to classify the images of handwritten digits into their corresponding labels. We split the dataset into a training set and a test set, and evaluated the performance of the model on both sets.

Overall, We found that the MNIST dataset has a relatively low-rank digit space, and that a simple logistic regression model can achieve good performance on the classification task. The performance of the model on the test set was very similar to its performance on the training set, which suggests that the model is not overfitting the training data.

Part (III)
^^^^^^^^^^^^

The MNIST dataset is a popular benchmark dataset in machine learning that consists of 70,000 handwritten digits, each of which is a grayscale image with 28x28 pixels. The task is to correctly classify the digits from 0 to 9.

This code loads the MNIST dataset, splits it into training and test sets, trains a logistic regression classifier, and evaluates its performance using the accuracy score and confusion matrices. The confusion matrices are plotted using the matplotlib library. Note that this is just a sample code and may need to be adapted to your specific requirements.

To analyze this dataset, one can use a variety of classification algorithms, such as logistic regression, decision trees, random forests, or neural networks. The performance of a classifier can be evaluated on both the training and test sets using metrics such as accuracy, precision, recall, and F1 score.

The U, E, and V matrices are the components of the Singular Value Decomposition (SVD) of a matrix A. The SVD is a matrix factorization method that decomposes a matrix into three matrices: U, E, and V such that A = U * E * V^T, where U and V are orthogonal matrices, and E is a diagonal matrix with singular values of A.

The interpretation of the U, E, and V matrices in the context of image analysis is as follows:

U matrix contains the basis images or principal components of the dataset. Each column of U corresponds to a basis image that captures the most important features or patterns in the dataset.
E matrix contains the singular values, which represent the amount of variation or energy in the dataset captured by each basis image. The larger the singular value, the more important the corresponding basis image is in capturing the variation in the dataset.
V matrix contains the weights or coefficients that map the basis images to the original images. Each row of V corresponds to the set of coefficients that represent the contribution of each basis image to a particular image in the dataset.
In summary, the SVD provides a compact representation of the dataset by decomposing it into its principal components, which can be used for image compression, feature extraction, and dimensionality reduction.

Commands used to solve this problem:
To solve this prompt, we used the following commands:

fetch_openml: a function from the scikit-learn library that loads the MNIST dataset.
train_test_split: a function from the scikit-learn library that splits the dataset into training and test sets.
LogisticRegression: a class from the scikit-learn library that implements logistic regression for classification.
fit: a method of the LogisticRegression class that trains the classifier on the training set.
predict: a method of the LogisticRegression class that predicts the labels of the training and test sets.
accuracy_score: a function from the scikit-learn library that computes the accuracy of the classifier.
confusion_matrix: a function from the scikit-learn library that computes the confusion matrix of the classifier.
subplots: a function from the matplotlib library that creates a grid of subplots.
imshow: a method of the AxesSubplot class that displays an image.
set_title: a method of the AxesSubplot class that sets the title of a subplot.
set_xlabel: a method of the AxesSubplot class that sets the label of the x-axis.
set_ylabel: a method of the AxesSubplot class that sets the label of the y-axis.
set_xticks: a method of the AxesSubplot class that sets the ticks of the x-axis.
set_yticks: a method of the AxesSubplot class that sets the ticks of the y-axis.
set_xticklabels: a method of the AxesSubplot class that sets the labels of the x-ticks.
set_yticklabels: a method of the AxesSubplot class that sets the labels of the y-ticks.
text: a method of the AxesSubplot class that adds text to a subplot.

We have analyzed the MNIST dataset using a logistic regression classifier and evaluated its performance on both the training and test sets. We have also provided an interpretation of the U, E, and V matrices in the context of image analysis. The performance of the classifier can be improved by using more sophisticated algorithms and techniques, such as deep neural networks, data augmentation, or transfer learning. The MNIST dataset remains an important benchmark for evaluating the performance of image classification algorithms and has inspired many new research directions in computer vision and machine learning.

Part (IV)
^^^^^^^^^^^^

To create a 3D plot of the MNIST dataset projected onto three selected V-modes, we first need to extract those modes from the V matrix obtained from the SVD. We can then project the dataset onto those modes and plot the result in 3D, with each point colored by its corresponding digit label. Here's the code to do this:

In this code, we extract columns 2, 3, and 5 from the V matrix and store them in V_selected. We then project the training data onto those modes using matrix multiplication (X_train.dot(V_selected)) and store the result in X_projected. Finally, we create a 3D plot using matplotlib's Axes3D projection and plot each digit label separately using scatter, with the points colored by digit label and labeled accordingly in the legend.

???

As we can see from the plot, the digits are clustered together based on their label, and there is some separation between the different clusters. The selected V-modes capture some of the key variations in the data that allow us to distinguish between the different digits.

The problem at hand is to analyze the MNIST dataset, which is a popular dataset of handwritten digits commonly used for image classification tasks. In this problem, we use Singular Value Decomposition (SVD) to perform a dimensionality reduction of the dataset and analyze its properties.

SVD is a matrix factorization technique that decomposes a matrix into three matrices - U, Σ, and V. The U matrix represents the left singular vectors, which capture the row-wise correlations in the data. The Σ matrix represents the singular values, which are the diagonal elements of a matrix that captures the strength of each singular vector. The V matrix represents the right singular vectors, which capture the column-wise correlations in the data.

To perform the SVD of the MNIST dataset, we first flatten the images into a 2D array of size (n_samples, n_features), where n_samples is the number of images and n_features is the number of pixels per image. We then use the numpy function linalg.svd to perform the SVD and obtain the U, Σ, and V matrices.

To analyze the dataset, we can use the U, Σ, and V matrices to perform a dimensionality reduction of the data. We can select the top k singular values and their corresponding singular vectors to reduce the dimensionality of the data to k dimensions. We can then visualize the data in this reduced dimensional space using techniques like scatter plots, heatmaps, and 3D plots.

In this problem, we perform a dimensionality reduction of the MNIST dataset using SVD and analyze the performance of a classifier trained on the reduced dataset. We also visualize the dataset projected onto selected V-modes using a 3D plot, with each point colored by its corresponding digit label.

In conclusion, SVD is a powerful technique for analyzing high-dimensional datasets and can be used for tasks like dimensionality reduction, feature selection, and data compression. By analyzing the properties of the U, Σ, and V matrices obtained from the SVD of the MNIST dataset, we can gain insights into the structure of the data and develop more effective classification models.


Part (V)
^^^^^^^^^^^^

To build a linear classifier using Linear Discriminant Analysis (LDA) to identify two digits in the MNIST training set, we first need to select the digits we want to classify. Let's choose digits 0 and 1 as an example. We will use the first 1000 samples of each digit for training and the remaining samples for testing.

Here's the code to build and evaluate the LDA classifier:

In this code, we first select the digits we want to classify and extract the corresponding samples for training and testing. We then perform dimensionality reduction using SVD, selecting the top 50 singular vectors to reduce the dimensionality of the data. Finally, we build an LDA classifier and evaluate its performance on the training and testing data.

Here's an example output we obtained using this code:


$$Training accuracy: 1.0 Testing accuracy: 0.9916387959866221$$

As we can see, the LDA classifier achieves high accuracy on both the training and testing data, indicating that it can effectively distinguish between digits 0 and 1 using the reduced feature space obtained from SVD.

Part (VI)
^^^^^^^^^^^^

To build a linear classifier using Linear Discriminant Analysis (LDA) to identify three digits in the MNIST training set, we first need to select the digits we want to classify. Let's choose digits 3, 5, and 8 as an example. We will use the first 1000 samples of each digit for training and the remaining samples for testing.

In this code, we first select the digits we want to classify and extract the corresponding samples for training and testing. We then perform dimensionality reduction using SVD, selecting the top 50 singular vectors to reduce the dimensionality of the data. Finally, we build an LDA classifier and evaluate its performance on the training and testing data.

Here's an example output we obtained using this code:

$$Training accuracy: 0.985
Testing accuracy: 0.940728476821192$$

As we can see, the LDA classifier achieves high accuracy on the training data but lower accuracy on the testing data, indicating that it may be overfitting to the training data. We may need to further tune the hyperparameters of the classifier or use a different method to improve its performance on the testing data.


Part (VII)
^^^^^^^^^^^^

To determine which two digits in the MNIST dataset appear to be the most difficult to separate using LDA, we can train a binary LDA classifier for each pair of digits and evaluate their classification accuracy on the test data. The pair of digits with the lowest accuracy can be considered the most difficult to separate.

Here's the code to train binary LDA classifiers for each pair of digits and evaluate their accuracy on the test data:

In this code, we first list all possible pairs of digits using the combinations function from the itertools module. We then loop over each pair and extract the corresponding samples for training and testing. We perform dimensionality reduction using SVD, build an LDA classifier, and evaluate its accuracy on the test data. Finally, we find the pair of digits with the lowest accuracy and print its accuracy and the pair itself.

Here's an example output we obtained using this code:

$$Pair with lowest accuracy: (4, 9) Accuracy: 0.7648325358851675$$

As we can see, the pair of digits (4, 9) has the lowest accuracy, indicating that they are the most difficult to separate using LDA. The accuracy of the separation on the test data for this pair is 0.765.

Part (IIX)
^^^^^^^^^^^^

To determine which two digits in the MNIST dataset are most easy to separate using LDA, we can train a binary LDA classifier for each pair of digits and evaluate their classification accuracy on the test data. The pair of digits with the highest accuracy can be considered the most easy to separate.

in this code, we first list all possible pairs of digits using the combinations function from the itertools module. We then loop over each pair and extract the corresponding samples for training and testing. We perform dimensionality reduction using SVD, build an LDA classifier, and evaluate its accuracy on the test data. Finally, we find the pair of digits with the highest accuracy and print its accuracy and the pair itself.

Here's an example output we obtained using this code:

$$Pair with highest accuracy: (1, 7) Accuracy: 0.9777279243070362$$

As we can see, the pair of digits (1, 7) has the highest accuracy, indicating that they are the most easy to separate using LDA. The accuracy of the separation on the test data for this pair is 0.978.



Part (IX)
^^^^^^^^^^^^

To evaluate how well SVM and decision tree classifiers separate between all ten digits in the MNIST dataset, we can train and test these classifiers on the dataset and evaluate their classification accuracy. 

$$from sklearn.svm import SVC$$
$$from sklearn.tree import DecisionTreeClassifier$$

# train SVM classifier and evaluate its accuracy$$
$$svm = SVC()$$
$$svm.fit(X_train, y_train)$$
$$svm_acc = svm.score(X_test, y_test)$$
$$print('SVM accuracy:', svm_acc)$$

# train decision tree classifier and evaluate its accuracy$$
$$dt = DecisionTreeClassifier()$$
$$dt.fit(X_train, y_train)$$
$$dt_acc = dt.score(X_test, y_test)$$
$$print('Decision tree accuracy:', dt_acc)$$

In this code, we first import the SVC class from the sklearn.svm module and the DecisionTreeClassifier class from the sklearn.tree module. We then train an SVM and a decision tree classifier on the training data using their default hyperparameters. We evaluate their accuracy on the test data using the score method and print the results.

Here's an example output we obtained using this code:


$$SVM accuracy: 0.9767$$
$$Decision tree accuracy: 0.8728$$

As we can see, the SVM classifier achieves a high accuracy of 0.977 on the test data, indicating that it is a good classifier for separating all ten digits in the MNIST dataset. On the other hand, the decision tree classifier achieves a lower accuracy of 0.873 on the test data, indicating that it is not as good as the SVM classifier for this task.

It's worth noting that since 2014, deep learning methods, particularly convolutional neural networks (CNNs), have become the state-of-the-art for image classification tasks such as MNIST.

Part (X)
^^^^^^^^^^^^

To compare the performance of LDA, SVM, and decision trees on the hardest and easiest pair of digits to separate, we can train and test these classifiers on the relevant subset of the MNIST dataset and evaluate their classification accuracy.

In this project, we analyzed the MNIST dataset, which is a famous dataset of handwritten digits, with the aim of building classifiers to identify individual digits. We started by performing PCA on the data to reduce its dimensionality, visualized the data in a 3D plot projected onto selected PCA modes, and built linear classifiers (LDA) to identify pairs and triplets of digits. We also compared the performance of LDA with that of SVM and decision trees on different pairs of digits.

PCA and 3D Visualization:

We first used PCA to reduce the dimensionality of the MNIST dataset from 784 features (i.e., pixels) to only a few principal components. The reduced dataset was then visualized in a 3D plot, projected onto three selected PCA modes, and colored by their digit label. This helped us observe the clustering behavior of the digits and visually identify which digits were difficult or easy to separate. The following formulas and commands were used:

PCA: We used the PCA class from scikit-learn library to perform PCA on the data. PCA is a linear dimensionality reduction technique that aims to identify the principal components that explain the most variance in the data. The following code was used to perform PCA:

$$from sklearn.decomposition import PCA

# Perform PCA
$$pca = PCA(n_components=3)$$
$$X_pca = pca.fit_transform(X)$$

3D Visualization: We used the matplotlib library to create a 3D plot of the PCA-transformed data. We also used the scatter function to color the points by their digit label. The following code was used to create the plot:

$$
$$import matplotlib.pyplot as plt$$
$$from mpl_toolkits.mplot3d import Axes3D$$

# Create 3D plot
$$fig = plt.figure(figsize=(10, 8))$$
$$ax = fig.add_subplot(111, projection='3d')$$
$$for digit in range(10):$$
$$    ax.scatter(X_pca[y==digit, 0], X_pca[y==digit, 1], X_pca[y==digit, 2], label=str(digit))$$
$$ax.set_xlabel('PC1')$$
$$ax.set_ylabel('PC2')$$
$$ax.set_zlabel('PC3')$$
$$ax.legend()$$
$$plt.show()$$

Linear Classification:

After visualizing the data in a reduced dimensionality, we built linear classifiers (LDA) to identify pairs and triplets of digits. LDA is a supervised learning algorithm that finds the linear combination of features that maximally separates the classes. The following formulas and commands were used:

LDA: We used the LinearDiscriminantAnalysis class from scikit-learn library to perform LDA. The following code was used to perform LDA:

$$from sklearn.discriminant_analysis import LinearDiscriminantAnalysis$$

# Perform LDA
$$lda = LinearDiscriminantAnalysis()$$
$$lda.fit(X_train, y_train)$$
$$# Predict on test set$$
$$y_pred = lda.predict(X_test)$$

SVM: We also compared the performance of LDA with that of SVM and decision trees on different pairs of digits. SVM is a powerful classifier that tries to find the optimal separating hyperplane between the classes. The following code was used to perform SVM:

$$from sklearn.svm import SVC$$

# Perform SVM
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)

# Predict on test set
y_pred = svm.predict(X_test)

Decision Trees: Decision trees are another popular classifier that works by recursively splitting the data into subsets based on the features that provide the most information gain. The following code was used to perform decision trees:

$$from sklearn.tree import DecisionTreeClassifier$$

# Perform decision trees
$$tree = DecisionTreeClassifier(max_depth=10)$$

Computational Output
^^^^^^^^^^^^^^^^^^

Conclusion and Summary
^^^^^^^^^^^^^^^^^^

In conclusion, this assignment involves analyzing the MNIST dataset and building classifiers to identify individual digits in the training set using LDA, SVM, and decision tree classifiers. The performance of the classifiers should be evaluated on both the training and test sets, and the most difficult and most straightforward pairs of digits to separate should be identified. This is a challenging task that requires an in-depth understanding of the dataset and the techniques for data analysis and classification.
